import jsonlines
import sys
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import re, textwrap
#####################################################
# Please finish all TODOs in this file for MP1;
# do not change other code/formatting.
#####################################################
def get_first_function(code_string: str) -> str:
    """
    Extracts the first function definition from a string, including
    docstrings, comments, and its body, regardless of indentation.

    Args:
        code_string: The string containing the Python code.

    Returns:
        The full string of the first function found, including its body,
        or an empty string if no function is found.
    """
    lines = code_string.splitlines()
    function_lines = []

    # Find the first 'def' line and its indentation
    start_index = -1
    initial_indent = -1

    for i, line in enumerate(lines):
        trimmed_line = line.lstrip()
        if trimmed_line.startswith('def '):
            start_index = i
            initial_indent = len(line) - len(trimmed_line)
            break

    if start_index == -1:
        return "" # No function found

    # Add the function definition line
    function_lines.append(lines[start_index])

    # Capture the body of the function
    for i in range(start_index + 1, len(lines)):
        line = lines[i]
        trimmed_line = line.lstrip()

        # Stop at the first line that is neither blank nor indented more
        # than the function's starting line.
        if len(trimmed_line) > 0 and (len(line) - len(trimmed_line)) <= initial_indent:
            break

        function_lines.append(line)

    return "\n".join(function_lines)
_IMPORT_RE = re.compile(
    r'^\s*(?:from\s+[A-Za-z_][\w\.]*\s+import\s+[A-Za-z_\*][\w\*,\s\(\)]*'
    r'|import\s+[A-Za-z_][\w\.]*(?:\s*,\s*[A-Za-z_][\w\.]*)*)\s*(?:#.*)?$'
)

def collect_header_imports(src: str, start_line: int) -> str:
    """收集函式前的 imports"""
    if start_line < 0:
        return ""
    lines = src.splitlines()
    imports = []
    for i in range(start_line):  # 只看函式前面的行
        if _IMPORT_RE.match(lines[i]):
            imports.append(lines[i].strip())
    seen, dedup = set(), []
    for imp in imports:
        if imp not in seen:
            seen.add(imp)
            dedup.append(imp)
    return "\n".join(dedup)

def postprocess_keep_imports(response: str) -> str:
    """在不改動 get_first_function 的前提下，加上 header imports"""
    lines = response.splitlines()
    # 先抓函式
    fn_code = get_first_function(response)
    if not fn_code:
        return ""
    # 算出函式的起始行號（重用你的邏輯）
    start_index = -1
    for i, line in enumerate(lines):
        if line.lstrip().startswith('def '):
            start_index = i
            break
    header = collect_header_imports(response, start_index)
    return (header + "\n\n" + fn_code) if header else fn_code

def save_file(content, file_path):
    with open(file_path, 'w') as file:
        file.write(content)

def prompt_model(dataset, model_name = "deepseek-ai/deepseek-coder-6.7b-base", quantization = True):
    print(f"Working with {model_name} quantization {quantization}...")
    
    # TODO: download the model
    tok = AutoTokenizer.from_pretrained(model_name)
    
  
    if quantization:
        # TODO: load the model with quantization
        m = AutoModelForCausalLM.from_pretrained(
          model_name,
          load_in_4bit=True,
          torch_dtype=torch.bfloat16,
          device_map="auto"
        )
        
    else:
        # TODO: load the model without quantization
        m = AutoModelForCausalLM.from_pretrained(
          model_name,
          # load_in_4bit=True,
          torch_dtype=torch.bfloat16,
          device_map="auto"
        )
    device = m.device
    results = []
    results_processed = []
    for case in dataset:
        prompt = case['prompt']
        
        # TODO: prompt the model and get the response
        input = tok(prompt, return_tensors="pt").to(device)
        response = m.generate(**input, temperature=0, max_new_tokens=500)
        response = tok.decode(response[0], skip_special_tokens=True)
        
        print(f"Task_ID {case['task_id']}:\nPrompt:\n{prompt}\nResponse:\n{response}")
        results.append(dict(task_id=case["task_id"], completion=response))
        
        # TODO: postprocessing may be required to handle extraneous output, fix indentation issues generated by the model, or handle other similar, trivial issues
        
        
        response_processed = postprocess_keep_imports(response)
        results_processed.append(dict(task_id=case["task_id"], completion=response_processed))
    return results, results_processed

def read_jsonl(file_path):
    dataset = []
    with jsonlines.open(file_path) as reader:
        for line in reader: 
            dataset.append(line)
    return dataset

def write_jsonl(results, file_path):
    with jsonlines.open(file_path, "w") as f:
        for item in results:
            f.write_all([item])

if __name__ == "__main__":
    """
    This Python script is to run prompt LLMs for code synthesis.
    Usage:
    `python3 model_prompting.py <input_dataset> <model> <output_file> <output_file_processed> <if_quantization> `|& tee prompt.log

    Inputs:
    - <input_dataset>: A `.jsonl` file, which should be your team's dataset containing 20 HumanEval problems.
    - <model>: Specify the model to use. Options are "deepseek-ai/deepseek-coder-6.7b-base" or "deepseek-ai/deepseek-coder-6.7b-instruct".
    - <output_file>: A `.jsonl` file where the results will be saved.
    - <output_file_processed>: A `.jsonl` file where the processed results will be saved
    - <if_quantization>: Set to 'True' or 'False' to enable or disable model quantization.
    
    Outputs:
    - You can check <output_file> and  <output_file_processed> for detailed information.
    """
    args = sys.argv[1:]
    input_dataset = args[0]
    model = args[1]
    output_file = args[2]
    output_file_processed = args[3]
    if_quantization = args[4] # True or False
    
    if not input_dataset.endswith(".jsonl"):
        raise ValueError(f"{input_dataset} should be a `.jsonl` file!")
    
    if not output_file.endswith(".jsonl"):
        raise ValueError(f"{output_file} should be a `.jsonl` file!")
    
    quantization = True if if_quantization == "True" else False
    
    dataset = read_jsonl(input_dataset)
    results, results_processed = prompt_model(dataset, model, quantization)
    write_jsonl(results, output_file)
    write_jsonl(results_processed, output_file_processed)
