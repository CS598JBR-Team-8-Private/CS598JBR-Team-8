Reading samples...
0it [00:00, ?it/s]20it [00:00, 1095.98it/s]
Running test suites...
  0%|          | 0/20 [00:00<?, ?it/s] 35%|███▌      | 7/20 [00:00<00:00, 66.58it/s] 80%|████████  | 16/20 [00:00<00:00, 79.03it/s]100%|██████████| 20/20 [00:01<00:00, 19.09it/s]
[(0, {'task_id': 'HumanEval/121', 'passed': True, 'result': 'passed', 'completion_id': 0})]
[(0, {'task_id': 'HumanEval/27', 'passed': True, 'result': 'passed', 'completion_id': 0})]
[(0, {'task_id': 'HumanEval/91', 'passed': False, 'result': 'failed: Test 2', 'completion_id': 0})]
[(0, {'task_id': 'HumanEval/138', 'passed': False, 'result': 'failed: ', 'completion_id': 0})]
[(0, {'task_id': 'HumanEval/32', 'passed': False, 'result': 'failed: must be real number, not NoneType', 'completion_id': 0})]
[(0, {'task_id': 'HumanEval/113', 'passed': False, 'result': 'failed: Test 1', 'completion_id': 0})]
[(0, {'task_id': 'HumanEval/64', 'passed': False, 'result': 'failed: Test 2', 'completion_id': 0})]
[(0, {'task_id': 'HumanEval/45', 'passed': True, 'result': 'passed', 'completion_id': 0})]
[(0, {'task_id': 'HumanEval/0', 'passed': True, 'result': 'passed', 'completion_id': 0})]
[(0, {'task_id': 'HumanEval/153', 'passed': False, 'result': 'failed: ', 'completion_id': 0})]
[(0, {'task_id': 'HumanEval/35', 'passed': True, 'result': 'passed', 'completion_id': 0})]
[(0, {'task_id': 'HumanEval/145', 'passed': False, 'result': 'failed: ', 'completion_id': 0})]
[(0, {'task_id': 'HumanEval/105', 'passed': False, 'result': 'failed: Error', 'completion_id': 0})]
[(0, {'task_id': 'HumanEval/144', 'passed': False, 'result': 'failed: test1', 'completion_id': 0})]
[(0, {'task_id': 'HumanEval/114', 'passed': True, 'result': 'passed', 'completion_id': 0})]
[(0, {'task_id': 'HumanEval/157', 'passed': True, 'result': 'passed', 'completion_id': 0})]
[(0, {'task_id': 'HumanEval/75', 'passed': False, 'result': 'failed: ', 'completion_id': 0})]
[(0, {'task_id': 'HumanEval/150', 'passed': True, 'result': 'passed', 'completion_id': 0})]
[(0, {'task_id': 'HumanEval/111', 'passed': False, 'result': 'failed: This prints if this assert fails 1 (good for debugging!)', 'completion_id': 0})]
[(0, {'task_id': 'HumanEval/70', 'passed': True, 'result': 'passed', 'completion_id': 0})]
Writing results to base_prompt_processed_79244547625250131920467003550834601672.jsonl_results.jsonl...
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:00<00:05,  3.57it/s]100%|██████████| 20/20 [00:00<00:00, 71.34it/s]
{'pass@1': np.float64(0.45)}
